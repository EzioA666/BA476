{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CZiXWN60BGaO"},"source":["# BA 476 Lab 3: Cross-validation and intuition\n","\n","Today we will look at how to do cross-validation in Python and build some intuition about what happens when some of our implicit assumptions about our data breaks down. We will specifically look at what happens when we train on datasets that are too small or non-representative, and look at how to include interaction terms in our models."]},{"cell_type":"markdown","metadata":{"id":"cm6qfnWICL00"},"source":["## Background\n","\n","We will continue using the dataset provided by Cogo Labs that we've used in previous labs. Recall that we are trying to predict customers' email open rates."]},{"cell_type":"markdown","metadata":{"id":"LfKyubVDCL3V"},"source":["## Setup\n","\n","Lets start by importing the necessary libraries and mounting the Google Drive:"]},{"cell_type":"code","metadata":{"id":"GmMIgRcrbmmR","executionInfo":{"status":"ok","timestamp":1710775924163,"user_tz":240,"elapsed":21484,"user":{"displayName":"Gerdus Benade","userId":"10720838145833134874"}},"outputId":"ee10c255-bfe6-4f72-9aea-04f4897ec815","colab":{"base_uri":"https://localhost:8080/"}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Lasso, Ridge\n","\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.preprocessing import StandardScaler\n","\n","from sklearn.metrics import mean_squared_error\n","from sklearn.pipeline import make_pipeline\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"7SxVujQMDMGE"},"source":["### Import the data\n","\n","The dataset we will use is the same that we used for the labs on descriptive analytics; refer to the earlier\n","descriptions for details. Let’s load the training data. Change the path below to accurately reflect the location\n","of the data on your Drive."]},{"cell_type":"code","metadata":{"id":"dtEpjcayCmrm"},"source":["df = pd.read_csv('/content/drive/My Drive/ba476-test/data/cogo-all.tsv', sep='\\t')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rELTHzl38uaK"},"source":["### Train and Test Sets\n","\n","We start by splitting the data into a training and testing set. We've done this manually before, but today we'll use Scikit-learn's `train_test_split` function."]},{"cell_type":"code","metadata":{"id":"98JIjvUt8lzy"},"source":["predictors = [\"browser1\", \"browser2\", \"browser3\", 'activity_observations','activity_days', 'activity_recency', 'activity_locations' ]\n","#X = df.loc[:, df.columns != \"p_open\"]\n","X = df.loc[:, predictors]\n","y = df[\"p_open\"]\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zpNcsudC_UoF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709839275502,"user_tz":300,"elapsed":2,"user":{"displayName":"Gerdus Benade","userId":"10720838145833134874"}},"outputId":"c6a7ce3f-89ac-4615-9a42-78faf193a009"},"source":["X_train.shape, X_test.shape, y_train.shape, y_test.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((230638, 7), (57660, 7), (230638,), (57660,))"]},"metadata":{},"execution_count":87}]},{"cell_type":"markdown","metadata":{"id":"-sCK5_mbhu8C"},"source":["\n","Note that if you did not set your random seed (or set it to something else) your results may look different since we are training on randomly selected rows."]},{"cell_type":"markdown","metadata":{"id":"B_Po3aNajyEI"},"source":["## Lasso Regression (recap)\n","\n","We've already seen how to train a lasso regression model, inspect its coefficients and compute mean squared error."]},{"cell_type":"code","metadata":{"id":"yAnO8ZSEovoD"},"source":["# Create a lasso regression object\n","# Train the model\n","# Predict\n","# Evaluate"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cross-validation from scratch\n","\n","Implement cv for model evaluation to get a better estimate of your model's out of sample performance. Start with a function that returns the train and validation folds for each iteration."],"metadata":{"id":"x7GCKr5XDmV_"}},{"cell_type":"code","source":["# write a function that returns the training/validation folds"],"metadata":{"id":"J51Ob6wcXXJw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Model evaluation\n","Now we can do cross validation on our lasso model. Print the mse per iteration as well as the final estimate of performance."],"metadata":{"id":"5A382JWfUMHu"}},{"cell_type":"code","source":["# use your function to do cv for model evaluation on a lasso model"],"metadata":{"id":"cehMFQX4Xcc5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Parameter tuning\n","We can do something similar when tuning a parameter. Remember to evaluate your model on the test set after tuning the parameter."],"metadata":{"id":"__SjYvazJgOn"}},{"cell_type":"code","source":["# tune the alpha parameter of the lasso model"],"metadata":{"id":"5lZ3yvbrXiMN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# after selecting a parameter, evaluate the tuned model on the test set"],"metadata":{"id":"p2jBkoLwXnWD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ybGQ_tEPRnoL"},"source":["## Cross-validation in sklearn\n","We discussed how cross-validation gives us more accurate error estimates by repeatedly treating a different subset of our data as validation set. Scikit-learn has built-in functions for cross-validation.\n","\n","We will use the `cross_val_score` function, which has three required arguments when doing supervised learning: a classifier, your data (`X`), and your outcomes (`y`). The optional argument `cv` let's you set the number of folds you want to use.\n","The  `scoring` argument evaluates several known scoring rules automatically so that you don't have to compute the error rate by hand. The available scoring rules are discussed [here](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)."]},{"cell_type":"code","source":["#use cross_val_score to evaluate a linear regression model"],"metadata":{"id":"ami7nkNaXuSN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2NuBUld2TIjG"},"source":["Notice that it returns negative mean squared error by default (but this is easy enough to negate). Now that we have the accuracy on every fold, we can compute our final accuracy estimate."]},{"cell_type":"code","source":["#"],"metadata":{"id":"l6zGx13dX1Qu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h0Io1VClTlSk"},"source":["### Cross-validation for parameter selection\n","\n","Recall that Lasso and Ridge have a regularization parameter $\\lambda$ which must be tuned. One way to tune this would be to use the `cross_val_score` function several times, once for each of the parameter values you are considering.  You should try to implement this to make sure you understand the steps."]},{"cell_type":"code","source":["# use cross_val_score for parameter tuning"],"metadata":{"id":"vAHoYv7LX3a1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vDNwRQxYfEHX"},"source":["Now that we have our cross-validation estimates for each parameter, we should train a model on the entire training set using the best parameter. This is the model we will evaluate on the test set."]},{"cell_type":"markdown","source":["## LassoCV, RidgeCV"],"metadata":{"id":"uh1DC-AUQUB_"}},{"cell_type":"markdown","metadata":{"id":"XiXwkS85US5o"},"source":["The above process can be used to tune any type of model with hyperparameters. Tuning a lasso/ridge model is very common, so it has been automated in sklearn.  [`LassoCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) and [`RidgeCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)  allow you to specify a vector (or number) of $\\alpha$'s  to try as well as the number of folds used for the cross-validation.\n","\n","Repeat the tuning process above using `RidgeCV`."]},{"cell_type":"code","source":["#"],"metadata":{"id":"qJ8o6WHMX7yL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jjA5IIGu132k"},"source":["## Building intuition\n","\n","Spend the remainder of the lab investigating what happens when some of our common assumptions fail. In particular, test what happens when:\n","1. you train on a training set with a very different distribution from the testing set (for example by choosing to include only instances that ssatisfy a certain criteria in the training set). This will highlight the importance of randomness.\n","2. the size on the training set is very small (or more generally, how does the number of instances you train on influence the quality of the model). This highlights the importance of having enough data.\n","3. you add predictors to a model. This should highlight that additional predictors increase the flexibility of the model.\n","\n"]},{"cell_type":"markdown","source":["### Data preparation\n"," Let’s take a random subsample\n","of the training set to speed up training. Then, when we are happy with the tuning of our algorithms, we can increase\n","the size of the training set further."],"metadata":{"id":"yLbhiBeDWIz4"}},{"cell_type":"code","source":["X = df.loc[:, df.columns != \"p_open\"]\n","y = df[\"p_open\"]\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n","\n","df_train = X_train.copy()\n","\n","# Add p_open to the combined training dataframe\n","df_train[\"p_open\"] = y_train\n","\n","# Randomly sample 5000 rows from the training data\n","train_sample_size = 5000\n","df_train_sample = df_train.sample(n=train_sample_size, random_state=5)\n","\n","df_train_sample.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-XLnjO3cV72g","executionInfo":{"status":"ok","timestamp":1709839287459,"user_tz":300,"elapsed":184,"user":{"displayName":"Gerdus Benade","userId":"10720838145833134874"}},"outputId":"2ec9c533-a977-4b0e-b8c6-74df721aef99"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5000, 17)"]},"metadata":{},"execution_count":99}]},{"cell_type":"code","source":["# it's up to you to add more predictors as you see fit\n","predictors1 = [\"browser1\", \"browser2\", \"browser3\"]\n","\n","X_train_sample_p1 = df_train_sample[predictors1]\n","y_train_sample = df_train_sample[\"p_open\"][:, np.newaxis]\n","\n","X_test_p1 = X_test[predictors1]\n","y_train_sample.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FHa-qXS3WCHb","executionInfo":{"status":"ok","timestamp":1709839287459,"user_tz":300,"elapsed":5,"user":{"displayName":"Gerdus Benade","userId":"10720838145833134874"}},"outputId":"6d8b07a7-63ad-466d-90d6-57cf42baeeb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-100-9ee5428ce4b5>:5: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n","  y_train_sample = df_train_sample[\"p_open\"][:, np.newaxis]\n"]},{"output_type":"execute_result","data":{"text/plain":["(5000, 1)"]},"metadata":{},"execution_count":100}]}]}