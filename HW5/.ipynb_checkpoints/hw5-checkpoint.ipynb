{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  Age  Income  LoanAmount  CreditScore  MonthsEmployed  NumCreditLines   \n",
      "0   0   21   78304      168713          653              60               1  \\\n",
      "1   1   28   63751       84674          681              58               1   \n",
      "2   2   57   96676      167540          467              98               4   \n",
      "3   3   24   79289       61546          358              63               4   \n",
      "4   4   31   98586      232342          692              10               2   \n",
      "\n",
      "   InterestRate  LoanTerm  DTIRatio    Education EmploymentType MaritalStatus   \n",
      "0          8.80        60      0.59  High School      Part-time        Single  \\\n",
      "1          4.91        48      0.21          PhD      Part-time       Married   \n",
      "2         16.78        36      0.63  High School     Unemployed        Single   \n",
      "3          6.40        60      0.83     Master's      Full-time        Single   \n",
      "4         19.97        60      0.51          PhD     Unemployed       Married   \n",
      "\n",
      "  HasMortgage HasDependents LoanPurpose HasCoSigner  Default  \n",
      "0          No           Yes        Home         Yes        0  \n",
      "1         Yes           Yes        Auto          No        0  \n",
      "2          No           Yes    Business         Yes        0  \n",
      "3         Yes           Yes    Business         Yes        0  \n",
      "4         Yes           Yes   Education         Yes        0  \n",
      "Brier score loss: 0.11529765418234914\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "# Explore the data\n",
    "print(train_data.head())\n",
    "\n",
    "# Encode categorical variables using one-hot encoding\n",
    "categorical_cols = ['Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\n",
    "train_data_encoded = pd.get_dummies(train_data, columns=categorical_cols)\n",
    "\n",
    "# Separate features and target\n",
    "X = train_data_encoded.drop(['ID', 'Default'], axis=1)\n",
    "y = train_data_encoded['Default']\n",
    "\n",
    "# Preprocessing: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the validation set\n",
    "probabilities = model.predict_proba(X_val)[:, 1]  # get the probability of the positive class\n",
    "\n",
    "# Evaluate the model using Brier score loss\n",
    "brier_score = brier_score_loss(y_val, probabilities)\n",
    "print(f\"Brier score loss: {brier_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created.\n"
     ]
    }
   ],
   "source": [
    "# Assuming your preprocessing and model training steps are satisfactory and finalized.\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Apply the same preprocessing to the test data\n",
    "test_data_encoded = pd.get_dummies(test_data, columns=categorical_cols)\n",
    "# Ensure the test data has the same features as the training data, filling missing columns with zeros\n",
    "test_data_encoded = test_data_encoded.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "# Scale the test data using the same scaler used for the train data\n",
    "X_test_scaled = scaler.transform(test_data_encoded)\n",
    "\n",
    "# Predict probabilities with the trained model\n",
    "test_probabilities = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_data['ID'],\n",
    "    'TARGET': test_probabilities\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               4096      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,465\n",
      "Trainable params: 14,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 15:07:48.324738: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750/3750 [==============================] - 44s 11ms/step - loss: 0.3899 - accuracy: 0.8466 - auc: 0.7231 - val_loss: 0.3767 - val_accuracy: 0.8500 - val_auc: 0.7446\n",
      "Epoch 2/50\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 0.3817 - accuracy: 0.8487 - auc: 0.7389 - val_loss: 0.3753 - val_accuracy: 0.8502 - val_auc: 0.7484\n",
      "Epoch 3/50\n",
      "3750/3750 [==============================] - 43s 11ms/step - loss: 0.3793 - accuracy: 0.8492 - auc: 0.7439 - val_loss: 0.3738 - val_accuracy: 0.8507 - val_auc: 0.7526\n",
      "Epoch 4/50\n",
      "3750/3750 [==============================] - 43s 11ms/step - loss: 0.3770 - accuracy: 0.8505 - auc: 0.7479 - val_loss: 0.3741 - val_accuracy: 0.8509 - val_auc: 0.7509\n",
      "Epoch 5/50\n",
      "3750/3750 [==============================] - 44s 12ms/step - loss: 0.3764 - accuracy: 0.8502 - auc: 0.7492 - val_loss: 0.3728 - val_accuracy: 0.8508 - val_auc: 0.7534\n",
      "Epoch 6/50\n",
      "3750/3750 [==============================] - 43s 12ms/step - loss: 0.3750 - accuracy: 0.8512 - auc: 0.7514 - val_loss: 0.3719 - val_accuracy: 0.8508 - val_auc: 0.7550\n",
      "Epoch 7/50\n",
      "3750/3750 [==============================] - 43s 12ms/step - loss: 0.3742 - accuracy: 0.8513 - auc: 0.7535 - val_loss: 0.3725 - val_accuracy: 0.8513 - val_auc: 0.7542\n",
      "Epoch 8/50\n",
      "3750/3750 [==============================] - 43s 11ms/step - loss: 0.3734 - accuracy: 0.8512 - auc: 0.7543 - val_loss: 0.3778 - val_accuracy: 0.8498 - val_auc: 0.7542\n",
      "Epoch 9/50\n",
      "3750/3750 [==============================] - 43s 11ms/step - loss: 0.3729 - accuracy: 0.8519 - auc: 0.7547 - val_loss: 0.3751 - val_accuracy: 0.8512 - val_auc: 0.7548\n",
      "Epoch 10/50\n",
      "3750/3750 [==============================] - 43s 12ms/step - loss: 0.3723 - accuracy: 0.8518 - auc: 0.7567 - val_loss: 0.3733 - val_accuracy: 0.8508 - val_auc: 0.7539\n",
      "Epoch 11/50\n",
      "3750/3750 [==============================] - 43s 11ms/step - loss: 0.3716 - accuracy: 0.8526 - auc: 0.7577 - val_loss: 0.3715 - val_accuracy: 0.8500 - val_auc: 0.7556\n",
      "Epoch 12/50\n",
      "3750/3750 [==============================] - 43s 11ms/step - loss: 0.3712 - accuracy: 0.8521 - auc: 0.7584 - val_loss: 0.3723 - val_accuracy: 0.8512 - val_auc: 0.7533\n",
      "Epoch 13/50\n",
      "3750/3750 [==============================] - 43s 12ms/step - loss: 0.3708 - accuracy: 0.8524 - auc: 0.7588 - val_loss: 0.3721 - val_accuracy: 0.8508 - val_auc: 0.7561\n",
      "Epoch 14/50\n",
      "3750/3750 [==============================] - 44s 12ms/step - loss: 0.3702 - accuracy: 0.8526 - auc: 0.7603 - val_loss: 0.3715 - val_accuracy: 0.8518 - val_auc: 0.7565\n",
      "Epoch 15/50\n",
      "3750/3750 [==============================] - 43s 11ms/step - loss: 0.3697 - accuracy: 0.8530 - auc: 0.7607 - val_loss: 0.3718 - val_accuracy: 0.8515 - val_auc: 0.7557\n",
      "Epoch 16/50\n",
      "3750/3750 [==============================] - 43s 11ms/step - loss: 0.3692 - accuracy: 0.8534 - auc: 0.7613 - val_loss: 0.3732 - val_accuracy: 0.8507 - val_auc: 0.7549\n",
      "Epoch 17/50\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 0.3687 - accuracy: 0.8529 - auc: 0.7628 - val_loss: 0.3714 - val_accuracy: 0.8509 - val_auc: 0.7549\n",
      "Epoch 18/50\n",
      "3750/3750 [==============================] - 44s 12ms/step - loss: 0.3681 - accuracy: 0.8537 - auc: 0.7636 - val_loss: 0.3706 - val_accuracy: 0.8516 - val_auc: 0.7563\n",
      "Epoch 19/50\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 0.3672 - accuracy: 0.8536 - auc: 0.7650 - val_loss: 0.3716 - val_accuracy: 0.8512 - val_auc: 0.7551\n",
      "Epoch 20/50\n",
      "3750/3750 [==============================] - 43s 11ms/step - loss: 0.3669 - accuracy: 0.8541 - auc: 0.7653 - val_loss: 0.3717 - val_accuracy: 0.8503 - val_auc: 0.7563\n",
      "Epoch 21/50\n",
      "3750/3750 [==============================] - 43s 12ms/step - loss: 0.3663 - accuracy: 0.8547 - auc: 0.7654 - val_loss: 0.3710 - val_accuracy: 0.8507 - val_auc: 0.7559\n",
      "Epoch 22/50\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 0.3665 - accuracy: 0.8544 - auc: 0.7656 - val_loss: 0.3722 - val_accuracy: 0.8520 - val_auc: 0.7546\n",
      "Epoch 23/50\n",
      "3750/3750 [==============================] - 44s 12ms/step - loss: 0.3659 - accuracy: 0.8547 - auc: 0.7667 - val_loss: 0.3727 - val_accuracy: 0.8502 - val_auc: 0.7550\n",
      "Epoch 24/50\n",
      "3750/3750 [==============================] - 43s 12ms/step - loss: 0.3661 - accuracy: 0.8542 - auc: 0.7669 - val_loss: 0.3716 - val_accuracy: 0.8508 - val_auc: 0.7554\n",
      "Epoch 25/50\n",
      "3750/3750 [==============================] - 43s 12ms/step - loss: 0.3650 - accuracy: 0.8547 - auc: 0.7686 - val_loss: 0.3760 - val_accuracy: 0.8507 - val_auc: 0.7530\n",
      "Epoch 26/50\n",
      "3750/3750 [==============================] - 43s 12ms/step - loss: 0.3649 - accuracy: 0.8546 - auc: 0.7689 - val_loss: 0.3744 - val_accuracy: 0.8508 - val_auc: 0.7542\n",
      "Epoch 27/50\n",
      "3750/3750 [==============================] - 43s 12ms/step - loss: 0.3641 - accuracy: 0.8546 - auc: 0.7703 - val_loss: 0.3729 - val_accuracy: 0.8509 - val_auc: 0.7537\n",
      "Epoch 28/50\n",
      "3750/3750 [==============================] - 43s 12ms/step - loss: 0.3643 - accuracy: 0.8549 - auc: 0.7699 - val_loss: 0.3729 - val_accuracy: 0.8502 - val_auc: 0.7538\n",
      "Epoch 29/50\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 0.3633 - accuracy: 0.8556 - auc: 0.7712 - val_loss: 0.3770 - val_accuracy: 0.8509 - val_auc: 0.7516\n",
      "Epoch 30/50\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 0.3637 - accuracy: 0.8555 - auc: 0.7703 - val_loss: 0.3735 - val_accuracy: 0.8503 - val_auc: 0.7546\n",
      "Epoch 31/50\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 0.3627 - accuracy: 0.8559 - auc: 0.7726 - val_loss: 0.3739 - val_accuracy: 0.8509 - val_auc: 0.7553\n",
      "Epoch 32/50\n",
      "3750/3750 [==============================] - 44s 12ms/step - loss: 0.3629 - accuracy: 0.8555 - auc: 0.7721 - val_loss: 0.3736 - val_accuracy: 0.8497 - val_auc: 0.7532\n",
      "Epoch 33/50\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 0.3626 - accuracy: 0.8560 - auc: 0.7723 - val_loss: 0.3743 - val_accuracy: 0.8504 - val_auc: 0.7517\n",
      "Epoch 34/50\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 0.3617 - accuracy: 0.8565 - auc: 0.7740 - val_loss: 0.3750 - val_accuracy: 0.8504 - val_auc: 0.7508\n",
      "Epoch 35/50\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 0.3613 - accuracy: 0.8560 - auc: 0.7746 - val_loss: 0.3738 - val_accuracy: 0.8484 - val_auc: 0.7520\n",
      "Epoch 36/50\n",
      "3750/3750 [==============================] - 47s 13ms/step - loss: 0.3607 - accuracy: 0.8569 - auc: 0.7753 - val_loss: 0.3734 - val_accuracy: 0.8505 - val_auc: 0.7522\n",
      "Epoch 37/50\n",
      "3750/3750 [==============================] - 46s 12ms/step - loss: 0.3610 - accuracy: 0.8559 - auc: 0.7759 - val_loss: 0.3747 - val_accuracy: 0.8489 - val_auc: 0.7515\n",
      "Epoch 38/50\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 0.3609 - accuracy: 0.8565 - auc: 0.7753 - val_loss: 0.3761 - val_accuracy: 0.8505 - val_auc: 0.7525\n",
      "Epoch 39/50\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 0.3608 - accuracy: 0.8568 - auc: 0.7751 - val_loss: 0.3739 - val_accuracy: 0.8509 - val_auc: 0.7516\n",
      "Epoch 40/50\n",
      "3750/3750 [==============================] - 46s 12ms/step - loss: 0.3605 - accuracy: 0.8570 - auc: 0.7756 - val_loss: 0.3762 - val_accuracy: 0.8493 - val_auc: 0.7535\n",
      "Epoch 41/50\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 0.3598 - accuracy: 0.8569 - auc: 0.7769 - val_loss: 0.3753 - val_accuracy: 0.8491 - val_auc: 0.7496\n",
      "Epoch 42/50\n",
      "3750/3750 [==============================] - 46s 12ms/step - loss: 0.3596 - accuracy: 0.8575 - auc: 0.7770 - val_loss: 0.3761 - val_accuracy: 0.8490 - val_auc: 0.7514\n",
      "Epoch 43/50\n",
      "3750/3750 [==============================] - 43s 12ms/step - loss: 0.3596 - accuracy: 0.8575 - auc: 0.7772 - val_loss: 0.3737 - val_accuracy: 0.8498 - val_auc: 0.7529\n",
      "Epoch 44/50\n",
      "3750/3750 [==============================] - 44s 12ms/step - loss: 0.3586 - accuracy: 0.8572 - auc: 0.7784 - val_loss: 0.3751 - val_accuracy: 0.8497 - val_auc: 0.7505\n",
      "Epoch 45/50\n",
      "3750/3750 [==============================] - 44s 12ms/step - loss: 0.3585 - accuracy: 0.8584 - auc: 0.7786 - val_loss: 0.3746 - val_accuracy: 0.8503 - val_auc: 0.7509\n",
      "Epoch 46/50\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 0.3579 - accuracy: 0.8579 - auc: 0.7797 - val_loss: 0.3747 - val_accuracy: 0.8497 - val_auc: 0.7490\n",
      "Epoch 47/50\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 0.3585 - accuracy: 0.8575 - auc: 0.7796 - val_loss: 0.3767 - val_accuracy: 0.8492 - val_auc: 0.7487\n",
      "Epoch 48/50\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 0.3583 - accuracy: 0.8574 - auc: 0.7796 - val_loss: 0.3757 - val_accuracy: 0.8496 - val_auc: 0.7509\n",
      "Epoch 49/50\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 0.3575 - accuracy: 0.8578 - auc: 0.7808 - val_loss: 0.3746 - val_accuracy: 0.8499 - val_auc: 0.7500\n",
      "Epoch 50/50\n",
      "3750/3750 [==============================] - 44s 12ms/step - loss: 0.3577 - accuracy: 0.8576 - auc: 0.7807 - val_loss: 0.3760 - val_accuracy: 0.8485 - val_auc: 0.7496\n",
      "Validation Loss: 0.37599506974220276\n",
      "Validation Accuracy: 0.8485333323478699\n",
      "Validation AUC: 0.7496305108070374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['my_scaler.gz']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import joblib\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model configuration\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[BinaryAccuracy(name='accuracy'), tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy, val_auc = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f'Validation Loss: {val_loss}')\n",
    "print(f'Validation Accuracy: {val_accuracy}')\n",
    "print(f'Validation AUC: {val_auc}')\n",
    "\n",
    "model.save('model_1.h5')\n",
    "joblib.dump(scaler, 'my_scaler.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 3s 2ms/step\n",
      "Neural network submission file created.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the training data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "categorical_cols = ['Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\n",
    "train_data_encoded = pd.get_dummies(train_data, columns=categorical_cols)\n",
    "\n",
    "# Separate features and target\n",
    "X = train_data_encoded.drop(['ID', 'Default'], axis=1)\n",
    "y = train_data_encoded['Default']\n",
    "\n",
    "# Keep the feature names after encoding for later use\n",
    "feature_names = X.columns\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load model and scaler\n",
    "model = tf.keras.models.load_model('model_1.h5')\n",
    "scaler = joblib.load('my_scaler.gz')\n",
    "\n",
    "# Load and prepare the test data\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_data_encoded = pd.get_dummies(test_data, columns=categorical_cols)\n",
    "\n",
    "# Ensure the test data has the same features as the training data, filling missing columns with zeros\n",
    "test_data_encoded = test_data_encoded.reindex(columns=feature_names, fill_value=0)\n",
    "\n",
    "# Scale the test data using the same scaler used for the train data\n",
    "X_test_scaled = scaler.transform(test_data_encoded)\n",
    "\n",
    "# Predicting probabilities on the test set\n",
    "test_probabilities = model.predict(X_test_scaled).flatten()\n",
    "\n",
    "# Creating a submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_data['ID'],\n",
    "    'TARGET': test_probabilities\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv('neural_network_submission.csv', index=False)\n",
    "print(\"Neural network submission file created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Brier score loss: 0.11295756\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = ['Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\n",
    "numerical_cols = ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio']\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Append classifier to preprocessing pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))])\n",
    "\n",
    "# Separate features and target\n",
    "X = train_data.drop(['ID', 'Default'], axis=1)\n",
    "y = train_data['Default']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the validation set\n",
    "rf_probabilities = clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate the model using Brier score loss\n",
    "rf_brier_score = brier_score_loss(y_val, rf_probabilities)\n",
    "print(f\"Random Forest Brier score loss: {rf_brier_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Predict probabilities using the pipeline\n",
    "# The pipeline will automatically handle the preprocessing\n",
    "test_probabilities = clf.predict_proba(test_data.drop(['ID'], axis=1))[:, 1]\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_data['ID'],\n",
    "    'TARGET': test_probabilities\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv('random_forest.csv', index=False)\n",
    "print(\"Submission file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yunzheyu/miniconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brier score loss with AdaBoosted RandomForest: 0.11919698546731154\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = ['Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\n",
    "numerical_cols = ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio']\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Append classifier to preprocessing pipeline\n",
    "# RandomForest with AdaBoost\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=42)  # Using fewer trees for demonstration\n",
    "ada_boost = AdaBoostClassifier(base_estimator=rf, n_estimators=50, random_state=42)\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', ada_boost)])\n",
    "\n",
    "# Separate features and target\n",
    "X = train_data.drop(['ID', 'Default'], axis=1)\n",
    "y = train_data['Default']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the AdaBoosted RandomForest model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the validation set\n",
    "probabilities = clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate the model using Brier score loss\n",
    "brier_score = brier_score_loss(y_val, probabilities)\n",
    "print(f\"Brier score loss with AdaBoosted RandomForest: {brier_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
